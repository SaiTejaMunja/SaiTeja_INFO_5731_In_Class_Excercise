{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiTejaMunja/SaiTeja_INFO_5731_In_Class_Excercise/blob/main/Munja_Exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrdIpayY70dH"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORD0xJV_70dP"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijZw3iQC70dQ"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84cCLbhqSaEI",
        "outputId": "1efff530-4b15-4513-8007-c69091689c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrQoUebk70dT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb4a107-f0dd-4d09-a8b2-0d040c0ff842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics and their top words:\n",
            "(0, '0.026*\"product\" + 0.026*\"great\" + 0.026*\"could\" + 0.026*\"terrible\" + 0.026*\"today\" + 0.026*\"beach\" + 0.026*\"okay\" + 0.026*\"hate\" + 0.026*\"night\" + 0.026*\"masterpiece\"')\n",
            "(1, '0.026*\"product\" + 0.026*\"great\" + 0.026*\"could\" + 0.026*\"terrible\" + 0.026*\"hate\" + 0.026*\"perfect\" + 0.026*\"today\" + 0.026*\"okay\" + 0.026*\"book\" + 0.026*\"beach\"')\n",
            "(2, '0.095*\"could\" + 0.095*\"product\" + 0.095*\"put\" + 0.095*\"masterpiece\" + 0.095*\"book\" + 0.095*\"hate\" + 0.095*\"terrible\" + 0.011*\"great\" + 0.011*\"beach\" + 0.011*\"time\"')\n",
            "(3, '0.081*\"great\" + 0.081*\"city\" + 0.081*\"breathtaking\" + 0.081*\"beautiful\" + 0.081*\"friends\" + 0.081*\"time\" + 0.081*\"skyline\" + 0.081*\"beach\" + 0.081*\"night\" + 0.009*\"product\"')\n",
            "(4, '0.026*\"product\" + 0.026*\"great\" + 0.026*\"could\" + 0.026*\"terrible\" + 0.026*\"today\" + 0.026*\"beach\" + 0.026*\"hate\" + 0.026*\"book\" + 0.026*\"skyline\" + 0.026*\"perfect\"')\n",
            "(5, '0.087*\"night\" + 0.087*\"band\" + 0.087*\"concert\" + 0.087*\"played\" + 0.087*\"amazing\" + 0.087*\"last\" + 0.087*\"songs\" + 0.087*\"best\" + 0.010*\"great\" + 0.010*\"product\"')\n",
            "(6, '0.113*\"better\" + 0.060*\"product\" + 0.060*\"well\" + 0.060*\"hope\" + 0.060*\"feeling\" + 0.060*\"good\" + 0.060*\"love\" + 0.060*\"soon\" + 0.060*\"get\" + 0.060*\"really\"')\n",
            "(7, '0.087*\"today\" + 0.087*\"picnic\" + 0.087*\"lovely\" + 0.087*\"weather\" + 0.087*\"perfect\" + 0.087*\"okay\" + 0.087*\"great\" + 0.087*\"product\" + 0.010*\"could\" + 0.010*\"terrible\"')\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "# Corpus\n",
        "texts = [\n",
        "    \"The product is really good. I love it! Not bad, but could be better. ðŸ˜Š\",\n",
        "    \"This product is terrible. I hate it!\",\n",
        "    \"It's an okay product, not great.\",\n",
        "    \"The weather is lovely today. Perfect for a picnic!\",\n",
        "    \"I had a great time at the beach with my friends.\",\n",
        "    \"This book is a masterpiece. I couldn't put it down.\",\n",
        "    \"The concert last night was amazing. The band played their best songs.\",\n",
        "    \"I'm not feeling well today. I hope I get better soon.\",\n",
        "    \"The city skyline at night is breathtaking. It's so beautiful.\",\n",
        "]\n",
        "\n",
        "# Preprocessing the Corpus obtained from earlier asssignment\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "    tokens = [token for token in tokens if len(token) > 1]  # Remove single-character words\n",
        "    tokens = [token for token in tokens if not token.isdigit()]  # Remove numbers\n",
        "    tokens = [token for token in tokens if token.isalpha()]  # Remove punctuation\n",
        "    tokens = [token.lower() for token in tokens]  # Convert to lowercase\n",
        "    return tokens\n",
        "\n",
        "processed_texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "# Creating a dictionary and a corpus\n",
        "id2word = corpora.Dictionary(processed_texts)\n",
        "corpus = [id2word.doc2bow(text) for text in processed_texts]\n",
        "\n",
        "# Finding the optimal number of topics using coherence scores\n",
        "coherence_scores = {}\n",
        "start_topic, end_topic = 2, 10\n",
        "\n",
        "for num_topics in range(start_topic, end_topic + 1):\n",
        "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=id2word, passes=10)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores[num_topics] = coherence_score\n",
        "\n",
        "# Finding the number of topics with the highest coherence score\n",
        "optimal_num_topics = max(coherence_scores, key=coherence_scores.get)\n",
        "\n",
        "# Training the final LDA model with the optimal number of topics\n",
        "lda_model = gensim.models.LdaModel(corpus, num_topics=optimal_num_topics, id2word=id2word, passes=10)\n",
        "\n",
        "# Printing the topics and their top words\n",
        "topics = lda_model.print_topics()\n",
        "print(f\"Topics and their top words:\")\n",
        "for topic in topics:\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy8yyAjS70dW"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "texts = [\n",
        "    \"The product is really good. I love it! Not bad, but could be better. ðŸ˜Š\",\n",
        "    \"This product is terrible. I hate it!\",\n",
        "    \"It's an okay product, not great.\",\n",
        "    \"The weather is lovely today. Perfect for a picnic!\",\n",
        "    \"I had a great time at the beach with my friends.\",\n",
        "    \"This book is a masterpiece. I couldn't put it down.\",\n",
        "    \"The concert last night was amazing. The band played their best songs.\",\n",
        "    \"I'm not feeling well today. I hope I get better soon.\",\n",
        "    \"The city skyline at night is breathtaking. It's so beautiful.\",\n",
        "]\n",
        "\n",
        "# Preprocessing the text data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "    tokens = [token for token in tokens if len(token) > 1]  # Remove single-character words\n",
        "    tokens = [token for token in tokens if not token.isdigit()]  # Remove numbers\n",
        "    tokens = [token for token in tokens if token.isalpha()]  # Remove punctuation\n",
        "    tokens = [token.lower() for token in tokens]  # Convert to lowercase\n",
        "    return tokens\n",
        "\n",
        "processed_texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "# Creating a dictionary and a corpus\n",
        "id2word = corpora.Dictionary(processed_texts)\n",
        "corpus = [id2word.doc2bow(text) for text in processed_texts]\n",
        "\n",
        "# Determining the optimal number of topics using coherence score\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
        "    coherence_values = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LsiModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
        "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_score = coherence_model.get_coherence()\n",
        "        coherence_values.append((num_topics, coherence_score))\n",
        "    return coherence_values\n",
        "\n",
        "limit = 10\n",
        "start = 2\n",
        "step = 1\n",
        "coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=processed_texts, start=start, limit=limit, step=step)\n",
        "\n",
        "# Finding the number of topics with the highest coherence score\n",
        "best_num_topics = max(coherence_values, key=lambda x: x[1])[0]\n",
        "\n",
        "# Training the LSA model with the best number of topics\n",
        "lsa_model = LsiModel(corpus=corpus, id2word=id2word, num_topics=best_num_topics)\n",
        "\n",
        "# Printing the best number of topics\n",
        "print(\"Optimal number of topics:\", best_num_topics)\n",
        "\n",
        "# Printing the topics and their top words\n",
        "topics = lsa_model.print_topics()\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubghWC6BafkD",
        "outputId": "0ee2a659-a4a6-4e1e-9de1-b528d94cb055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal number of topics: 9\n",
            "(0, '0.455*\"better\" + 0.363*\"product\" + 0.305*\"could\" + 0.264*\"today\" + 0.250*\"bad\" + 0.250*\"love\" + 0.250*\"really\" + 0.250*\"good\" + 0.205*\"get\" + 0.205*\"feeling\"')\n",
            "(1, '-0.433*\"night\" + -0.332*\"songs\" + -0.332*\"concert\" + -0.332*\"played\" + -0.332*\"last\" + -0.332*\"amazing\" + -0.332*\"band\" + -0.332*\"best\" + -0.101*\"city\" + -0.101*\"breathtaking\"')\n",
            "(2, '0.409*\"today\" + -0.350*\"product\" + -0.269*\"could\" + 0.263*\"get\" + 0.263*\"hope\" + 0.263*\"soon\" + 0.263*\"well\" + 0.263*\"feeling\" + -0.199*\"really\" + -0.199*\"love\"')\n",
            "(3, '0.442*\"beautiful\" + 0.442*\"city\" + 0.442*\"breathtaking\" + 0.442*\"skyline\" + 0.308*\"night\" + -0.134*\"concert\" + -0.134*\"amazing\" + -0.134*\"played\" + -0.134*\"songs\" + -0.134*\"band\"')\n",
            "(4, '-0.502*\"great\" + -0.301*\"time\" + -0.301*\"friends\" + -0.301*\"beach\" + -0.243*\"weather\" + -0.243*\"picnic\" + -0.243*\"lovely\" + -0.243*\"perfect\" + -0.235*\"product\" + -0.201*\"okay\"')\n",
            "(5, '0.347*\"perfect\" + 0.347*\"weather\" + 0.347*\"picnic\" + 0.347*\"lovely\" + -0.305*\"great\" + -0.201*\"friends\" + -0.201*\"time\" + -0.201*\"beach\" + 0.196*\"could\" + 0.180*\"today\"')\n",
            "(6, '0.430*\"put\" + 0.430*\"book\" + 0.430*\"masterpiece\" + 0.346*\"could\" + -0.280*\"product\" + 0.207*\"beach\" + 0.207*\"time\" + 0.207*\"friends\" + 0.167*\"great\" + -0.155*\"hate\"')\n",
            "(7, '0.404*\"terrible\" + 0.404*\"hate\" + 0.389*\"product\" + -0.217*\"good\" + -0.217*\"really\" + -0.217*\"bad\" + -0.217*\"love\" + 0.209*\"put\" + 0.209*\"book\" + 0.209*\"masterpiece\"')\n",
            "(8, '0.568*\"okay\" + -0.430*\"terrible\" + -0.430*\"hate\" + 0.316*\"great\" + -0.252*\"friends\" + -0.252*\"time\" + -0.252*\"beach\" + 0.109*\"product\" + -0.030*\"love\" + -0.030*\"bad\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvmQsDtE70dX"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lda2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDIhAPTZe626",
        "outputId": "e07998d9-9d04-4bd4-c88a-4d1e3a4c8936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lda2vec in /usr/local/lib/python3.10/dist-packages (0.16.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5JEfiQiqL0i",
        "outputId": "173c5e3d-15c7-47ec-a29d-7d592df5fc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXQqVF9FfzfY",
        "outputId": "8b480b16-aa04-4769-ee2b-f839ed60a7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.3)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.1.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.8.7)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install preprocess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CHVYv6C2Cqq",
        "outputId": "c04ce402-9d82-4d0b-a477-b0b7c4e92d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: preprocess in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from preprocess) (0.18.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imporitng necessary libraries\n",
        "\n",
        "import pyLDAvis\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VowkUI-616dE",
        "outputId": "ee22f4c7-1941-456e-e18e-4f9f5ea784c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top = 10\n",
        "topic_to_topwords = {}\n",
        "\n",
        "texts = [\n",
        "    \"The product is really good. I love it! Not bad, but could be better. ðŸ˜Š\",\n",
        "    \"This product is terrible. I hate it!\",\n",
        "    \"It's an okay product, not great.\",\n",
        "    \"The weather is lovely today. Perfect for a picnic!\",\n",
        "    \"I had a great time at the beach with my friends.\",\n",
        "    \"This book is a masterpiece. I couldn't put it down.\",\n",
        "    \"The concert last night was amazing. The band played their best songs.\",\n",
        "    \"I'm not feeling well today. I hope I get better soon.\",\n",
        "    \"The city skyline at night is breathtaking. It's so beautiful.\",\n",
        "]\n",
        "\n",
        "for k, topic in enumerate(texts):\n",
        "    # Tokenizing the text and calculating the top words\n",
        "    words = nltk.word_tokenize(topic)\n",
        "    word_freq = nltk.FreqDist(words)\n",
        "    top_words = [word for word, freq in word_freq.most_common(top)]\n",
        "\n",
        "    msg = 'Topic %i has top words: %s' % (k, ', '.join(top_words))\n",
        "    print(msg)\n",
        "    topic_to_topwords[k] = top_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDw_7a0x2mUS",
        "outputId": "6ad1098c-4027-405b-9949-f3f2332b001b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0 has top words: ., The, product, is, really, good, I, love, it, !\n",
            "Topic 1 has top words: This, product, is, terrible, ., I, hate, it, !\n",
            "Topic 2 has top words: It, 's, an, okay, product, ,, not, great, .\n",
            "Topic 3 has top words: The, weather, is, lovely, today, ., Perfect, for, a, picnic\n",
            "Topic 4 has top words: I, had, a, great, time, at, the, beach, with, my\n",
            "Topic 5 has top words: ., This, book, is, a, masterpiece, I, could, n't, put\n",
            "Topic 6 has top words: The, ., concert, last, night, was, amazing, band, played, their\n",
            "Topic 7 has top words: I, ., 'm, not, feeling, well, today, hope, get, better\n",
            "Topic 8 has top words: ., The, city, skyline, at, night, is, breathtaking, It, 's\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6dQh5JT70dZ"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics Kshould be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvJ9_5xQSn2w",
        "outputId": "85523987-09f4-4693-90d7-e2abe532f76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.26.1)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.33)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.1.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.36)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (23.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.35.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.16.0+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.17.3)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.10)\n",
            "Requirement already satisfied: tbb>=2019.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (2021.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HBy9L6s3azL",
        "outputId": "be62808d-7bc2-4845-f287-de51249a8d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing librarbies\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Defining some list of sentences for creating a corpus\n",
        "sentences = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"Here is the second sentence.\",\n",
        "    \"A third sentence for testing.\",\n",
        "    \"And a fourth sentence for variety.\",\n",
        "    \"Fifth sentence, just to add more data.\",\n",
        "    \"Another sentence to make it six.\",\n",
        "    \"Seventh sentence, almost there.\",\n",
        "    \"Eighth sentence for the example.\",\n",
        "    \"Ninth sentence, we're getting closer.\",\n",
        "    \"Tenth sentence, halfway through.\",\n",
        "    \"Eleventh sentence to keep going.\",\n",
        "    \"Twelfth sentence, still more to come.\",\n",
        "    \"Thirteenth sentence, lucky number.\",\n",
        "    \"Fourteenth sentence, almost done.\",\n",
        "    \"Fifteenth sentence, just a few more.\",\n",
        "    \"Sixteenth sentence, getting there.\",\n",
        "    \"Seventeenth sentence, almost finished.\",\n",
        "    \"Eighteenth sentence, so close.\",\n",
        "    \"Nineteenth sentence, penultimate.\",\n",
        "    \"Twentieth sentence, last one.\",\n",
        "    \"Twenty-first sentence, the first of the second set.\",\n",
        "    \"Twenty-second sentence, continuing the second set.\",\n",
        "    \"Twenty-third sentence, adding more.\",\n",
        "    \"Twenty-fourth sentence, almost done with the second set.\",\n",
        "    \"Twenty-fifth sentence, last of the second set.\",\n",
        "    \"Twenty-sixth sentence, beginning the third set.\",\n",
        "    \"Twenty-seventh sentence, ongoing.\",\n",
        "    \"Twenty-eighth sentence, more to go.\",\n",
        "    \"Twenty-ninth sentence, not stopping yet.\",\n",
        "    \"Thirtieth sentence, third set's halfway point.\",\n",
        "    \"Thirty-first sentence, picking up speed.\",\n",
        "    \"Thirty-second sentence, still more to come.\",\n",
        "    \"Thirty-third sentence, almost there.\",\n",
        "    \"Thirty-fourth sentence, getting closer.\",\n",
        "    \"Thirty-fifth sentence, close to the end.\",\n",
        "    \"Thirty-sixth sentence, wrapping up the third set.\",\n",
        "    \"Thirty-seventh sentence, starting the fourth set.\",\n",
        "    \"Thirty-eighth sentence, not stopping now.\",\n",
        "    \"Thirty-ninth sentence, fourth set's halfway point.\",\n",
        "    \"Fortieth sentence, making progress.\",\n",
        "    \"Forty-first sentence, keeping going.\",\n",
        "    \"Forty-second sentence, just a few more to go.\",\n",
        "    \"Forty-third sentence, almost there.\",\n",
        "    \"Forty-fourth sentence, getting closer to the end.\",\n",
        "    \"Forty-fifth sentence, almost done.\",\n",
        "    \"Forty-sixth sentence, penultimate in the fourth set.\",\n",
        "    \"Forty-seventh sentence, last one in the fourth set.\",\n",
        "    \"Forty-eighth sentence, starting the fifth set.\",\n",
        "    \"Forty-ninth sentence, not stopping now.\",\n",
        "    \"Fiftieth sentence, last one for the example.\",\n",
        "]\n",
        "\n",
        "# Create a DataFrame with a column containing the sentences\n",
        "data_frame = pd.DataFrame({'Sentences': sentences})\n",
        "\n",
        "# Print the DataFrame\n",
        "print(data_frame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WMs9oSEBO_r",
        "outputId": "00d63705-e3f5-4e7b-ebc2-b9dd1fa03a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Sentences\n",
            "0                         This is the first sentence.\n",
            "1                        Here is the second sentence.\n",
            "2                       A third sentence for testing.\n",
            "3                  And a fourth sentence for variety.\n",
            "4              Fifth sentence, just to add more data.\n",
            "5                    Another sentence to make it six.\n",
            "6                     Seventh sentence, almost there.\n",
            "7                    Eighth sentence for the example.\n",
            "8               Ninth sentence, we're getting closer.\n",
            "9                    Tenth sentence, halfway through.\n",
            "10                   Eleventh sentence to keep going.\n",
            "11              Twelfth sentence, still more to come.\n",
            "12                 Thirteenth sentence, lucky number.\n",
            "13                  Fourteenth sentence, almost done.\n",
            "14               Fifteenth sentence, just a few more.\n",
            "15                 Sixteenth sentence, getting there.\n",
            "16             Seventeenth sentence, almost finished.\n",
            "17                     Eighteenth sentence, so close.\n",
            "18                  Nineteenth sentence, penultimate.\n",
            "19                      Twentieth sentence, last one.\n",
            "20  Twenty-first sentence, the first of the second...\n",
            "21  Twenty-second sentence, continuing the second ...\n",
            "22                Twenty-third sentence, adding more.\n",
            "23  Twenty-fourth sentence, almost done with the s...\n",
            "24     Twenty-fifth sentence, last of the second set.\n",
            "25    Twenty-sixth sentence, beginning the third set.\n",
            "26                  Twenty-seventh sentence, ongoing.\n",
            "27                Twenty-eighth sentence, more to go.\n",
            "28           Twenty-ninth sentence, not stopping yet.\n",
            "29     Thirtieth sentence, third set's halfway point.\n",
            "30           Thirty-first sentence, picking up speed.\n",
            "31        Thirty-second sentence, still more to come.\n",
            "32               Thirty-third sentence, almost there.\n",
            "33            Thirty-fourth sentence, getting closer.\n",
            "34           Thirty-fifth sentence, close to the end.\n",
            "35  Thirty-sixth sentence, wrapping up the third set.\n",
            "36  Thirty-seventh sentence, starting the fourth set.\n",
            "37          Thirty-eighth sentence, not stopping now.\n",
            "38  Thirty-ninth sentence, fourth set's halfway po...\n",
            "39                Fortieth sentence, making progress.\n",
            "40               Forty-first sentence, keeping going.\n",
            "41      Forty-second sentence, just a few more to go.\n",
            "42                Forty-third sentence, almost there.\n",
            "43  Forty-fourth sentence, getting closer to the end.\n",
            "44                 Forty-fifth sentence, almost done.\n",
            "45  Forty-sixth sentence, penultimate in the fourt...\n",
            "46  Forty-seventh sentence, last one in the fourth...\n",
            "47     Forty-eighth sentence, starting the fifth set.\n",
            "48            Forty-ninth sentence, not stopping now.\n",
            "49       Fiftieth sentence, last one for the example.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXqFpeeFIOdY",
        "outputId": "35317b3d-c389-4790-eda4-dc20ffe0fe01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "\n",
        "corpus = data_frame.Sentences.to_list()\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "topics, probs = topic_model.fit_transform(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "924721d9438e44faa8d0c3d9816af9a4",
            "92e22a611ced4540aea7ed59b219484a",
            "9adc1f4c8b694beeac19b65b3f9d2393",
            "f4100725c1614c31af386a9f30d32abe",
            "987dbfee7c534fe5af6b3b094c3506c1",
            "8a42e538a4fa458dbadb5aac813ebce0",
            "dfb9b65fb3774d8ba0a9b251d7e92352",
            "4420874b86db4be58bc61cdbcbd5930c",
            "b3f48c05ad1840cd9e952a1a841d5b5d",
            "d67ac00a27324f27b7bd8218df59ea51",
            "36780e88a0c741beaf20d1a3a50d8d54"
          ]
        },
        "id": "t9yD2rrhBKa_",
        "outputId": "5d4ff9d7-9983-4a84-8c3a-ed727e3a40af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "924721d9438e44faa8d0c3d9816af9a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-07 23:57:45,914 - BERTopic - Transformed documents to Embeddings\n",
            "2023-11-07 23:57:50,320 - BERTopic - Reduced dimensionality\n",
            "/usr/local/lib/python3.10/dist-packages/hdbscan/hdbscan_.py:1170: DeprecationWarning: `alltrue` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `all` instead.\n",
            "  self._all_finite = is_finite(X)\n",
            "2023-11-07 23:57:50,331 - BERTopic - Clustered reduced embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for topic, prob in zip(topics, probs):\n",
        "    print(f\"Topic: {topic}, Probability: {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-LTfXNwKdu8",
        "outputId": "d045656c-7bbe-4388-96a5-914d4d0e1e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 1, Probability: [5.96026351e-309 1.00000000e+000]\n",
            "Topic: -1, Probability: [0.14379035 0.66898313]\n",
            "Topic: -1, Probability: [0.20068501 0.61210311]\n",
            "Topic: 1, Probability: [4.13438674e-309 1.00000000e+000]\n",
            "Topic: 0, Probability: [0.39278415 0.49856449]\n",
            "Topic: 1, Probability: [6.28791633e-309 1.00000000e+000]\n",
            "Topic: 0, Probability: [0.73245176 0.19722432]\n",
            "Topic: -1, Probability: [0.22898339 0.52500104]\n",
            "Topic: 0, Probability: [1.00000000e+000 4.78521097e-309]\n",
            "Topic: 0, Probability: [0.61435095 0.27775248]\n",
            "Topic: 0, Probability: [0.60604337 0.31791562]\n",
            "Topic: 0, Probability: [1.00000000e+000 5.69318566e-309]\n",
            "Topic: -1, Probability: [0.29172695 0.4239999 ]\n",
            "Topic: -1, Probability: [0.23984051 0.46030979]\n",
            "Topic: -1, Probability: [0.31927874 0.39397789]\n",
            "Topic: -1, Probability: [0.38711595 0.37043326]\n",
            "Topic: -1, Probability: [0.24687979 0.45790246]\n",
            "Topic: -1, Probability: [0.21870974 0.66156848]\n",
            "Topic: 1, Probability: [6.18653171e-309 1.00000000e+000]\n",
            "Topic: 1, Probability: [5.98564394e-309 1.00000000e+000]\n",
            "Topic: -1, Probability: [0.08785416 0.7375442 ]\n",
            "Topic: -1, Probability: [0.13842542 0.69005929]\n",
            "Topic: -1, Probability: [0.36641774 0.44833576]\n",
            "Topic: 1, Probability: [4.74691089e-309 1.00000000e+000]\n",
            "Topic: 1, Probability: [4.36660233e-309 1.00000000e+000]\n",
            "Topic: 1, Probability: [4.28128072e-309 1.00000000e+000]\n",
            "Topic: -1, Probability: [0.34886993 0.44757539]\n",
            "Topic: 0, Probability: [0.56990935 0.35554711]\n",
            "Topic: -1, Probability: [0.50233073 0.36594491]\n",
            "Topic: -1, Probability: [0.14090633 0.73959745]\n",
            "Topic: 0, Probability: [1.00000000e+000 4.95478032e-309]\n",
            "Topic: 0, Probability: [1.00000000e+000 5.19358262e-309]\n",
            "Topic: 0, Probability: [1.00000000e+000 5.70573335e-309]\n",
            "Topic: 0, Probability: [1.00000000e+000 5.04614758e-309]\n",
            "Topic: 0, Probability: [1.00000000e+000 6.09831888e-309]\n",
            "Topic: 1, Probability: [4.52722553e-309 1.00000000e+000]\n",
            "Topic: -1, Probability: [0.09572497 0.75164055]\n",
            "Topic: 0, Probability: [0.73667046 0.24283373]\n",
            "Topic: -1, Probability: [0.11468296 0.65236334]\n",
            "Topic: -1, Probability: [0.57934701 0.26737636]\n",
            "Topic: 0, Probability: [0.63346662 0.26249859]\n",
            "Topic: 0, Probability: [1.00000000e+000 5.38470938e-309]\n",
            "Topic: 0, Probability: [1.00000000e+000 5.66154019e-309]\n",
            "Topic: 0, Probability: [1.00000000e+000 4.65070856e-309]\n",
            "Topic: 0, Probability: [0.76717717 0.15939776]\n",
            "Topic: -1, Probability: [0.1286813  0.65582091]\n",
            "Topic: -1, Probability: [0.0883332  0.66800274]\n",
            "Topic: 1, Probability: [3.78802077e-309 1.00000000e+000]\n",
            "Topic: -1, Probability: [0.58448958 0.27839992]\n",
            "Topic: -1, Probability: [0.24659235 0.53219687]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMFgdQmC70db"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEWwuuin70dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "687efda2-50a4-4bd5-af66-d3490f53f6c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"When comparing LDA (Latent Dirichlet Allocation) and LSA (Latent Semantic Analysis) for topic modeling, the choice depends on your specific needs. LDA offers interpretability with explicit topic-word probabilities and document-topic distributions, making it valuable when you want to understand the underlying topics in a corpus. LSA, on the other hand, excels at capturing semantic relationships and reducing dimensionality in the data but may lack the interpretability of LDA due to the absence of topic-word probabilities. The choice between the two algorithms should be driven by your project's objectives and whether you prioritize clear topic interpretation (LDA) or semantic similarity capture (LSA).\\n\\nIn comparison to LDA and LSA, BERT and Lda2Vec represent more advanced topic modeling approaches. BERT, a deep learning model, provides context-aware embeddings and can capture intricate semantic relationships but may require substantial computational resources. Lda2Vec combines the benefits of word2vec and LDA, offering topic interpretability while considering the distributional semantics of words. BERT and Lda2Vec are suitable for tasks requiring a deeper understanding of text data and context, although they can be computationally intensive. The choice between these methods, like LDA and LSA, depends on the specific project goals, resources, and the balance between interpretability and semantic understanding needed.\\n\\n\\nLatent Dirichlet Allocation (LDA):\\nLatent Dirichlet Allocation (LDA) is a well-established and widely used topic modeling technique. It provides interpretable topics by assigning a probability distribution of words to each topic. LDA is computationally efficient and works well with large text corpora. However, it assumes a bag-of-words model, which doesn't consider word order or semantics. This can lead to less accurate topic modeling, particularly in modern text data with complex language structures and semantics. LDA is suitable when you prioritize simplicity and have a large, well-structured corpus of text data where interpretability is crucial.\\n\\nLatent Semantic Analysis (LSA):\\nLatent Semantic Analysis (LSA) captures the underlying semantic structure in text data and is useful for reducing dimensionality in large datasets. LSA focuses on capturing latent semantic relationships among words and documents. However, LSA doesn't provide direct topic interpretability. It often requires additional techniques to identify and label topics. Additionally, LSA is limited in handling non-linear relationships in the data and struggles to capture word order and semantic nuances. LSA is a valuable choice when you need dimensionality reduction and have a large text dataset but don't require explicit topic labels.\\n\\nLDA2Vec:\\nLDA2Vec combines the strengths of LDA and word2vec to capture semantic relationships between words and documents. It can handle word order, semantics, and ambiguity more effectively than traditional LDA. This makes it suitable for tasks where capturing contextual information and semantics is essential. However, LDA2Vec may require more data and tuning to achieve optimal results. It can also be computationally intensive, making it less ideal for resource-constrained environments. LDA2Vec is a good choice when you want both interpretability and the ability to capture complex relationships in your text data.\\n\\nBERTopic (using BERT embeddings):\\nBERTopic leverages pre-trained BERT embeddings to capture contextual and semantic information in text data. It can handle word order, semantics, and ambiguity effectively, making it well-suited for modern text data with complex language structures. BERTopic is known for achieving high interpretability, even in short and noisy text data. However, it may require more computational resources due to the use of BERT embeddings. BERTopic is a strong choice when you prioritize topic interpretability and need to capture the nuanced meaning of words and phrases in your text data, making it a valuable option for a wide range of NLP tasks.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "\n",
        "'''When comparing LDA (Latent Dirichlet Allocation) and LSA (Latent Semantic Analysis) for topic modeling, the choice depends on your specific needs. LDA offers interpretability with explicit topic-word probabilities and document-topic distributions, making it valuable when you want to understand the underlying topics in a corpus. LSA, on the other hand, excels at capturing semantic relationships and reducing dimensionality in the data but may lack the interpretability of LDA due to the absence of topic-word probabilities. The choice between the two algorithms should be driven by your project's objectives and whether you prioritize clear topic interpretation (LDA) or semantic similarity capture (LSA).\n",
        "\n",
        "In comparison to LDA and LSA, BERT and Lda2Vec represent more advanced topic modeling approaches. BERT, a deep learning model, provides context-aware embeddings and can capture intricate semantic relationships but may require substantial computational resources. Lda2Vec combines the benefits of word2vec and LDA, offering topic interpretability while considering the distributional semantics of words. BERT and Lda2Vec are suitable for tasks requiring a deeper understanding of text data and context, although they can be computationally intensive. The choice between these methods, like LDA and LSA, depends on the specific project goals, resources, and the balance between interpretability and semantic understanding needed.\n",
        "\n",
        "\n",
        "Latent Dirichlet Allocation (LDA):\n",
        "Latent Dirichlet Allocation (LDA) is a well-established and widely used topic modeling technique. It provides interpretable topics by assigning a probability distribution of words to each topic. LDA is computationally efficient and works well with large text corpora. However, it assumes a bag-of-words model, which doesn't consider word order or semantics. This can lead to less accurate topic modeling, particularly in modern text data with complex language structures and semantics. LDA is suitable when you prioritize simplicity and have a large, well-structured corpus of text data where interpretability is crucial.\n",
        "\n",
        "Latent Semantic Analysis (LSA):\n",
        "Latent Semantic Analysis (LSA) captures the underlying semantic structure in text data and is useful for reducing dimensionality in large datasets. LSA focuses on capturing latent semantic relationships among words and documents. However, LSA doesn't provide direct topic interpretability. It often requires additional techniques to identify and label topics. Additionally, LSA is limited in handling non-linear relationships in the data and struggles to capture word order and semantic nuances. LSA is a valuable choice when you need dimensionality reduction and have a large text dataset but don't require explicit topic labels.\n",
        "\n",
        "LDA2Vec:\n",
        "LDA2Vec combines the strengths of LDA and word2vec to capture semantic relationships between words and documents. It can handle word order, semantics, and ambiguity more effectively than traditional LDA. This makes it suitable for tasks where capturing contextual information and semantics is essential. However, LDA2Vec may require more data and tuning to achieve optimal results. It can also be computationally intensive, making it less ideal for resource-constrained environments. LDA2Vec is a good choice when you want both interpretability and the ability to capture complex relationships in your text data.\n",
        "\n",
        "BERTopic (using BERT embeddings):\n",
        "BERTopic leverages pre-trained BERT embeddings to capture contextual and semantic information in text data. It can handle word order, semantics, and ambiguity effectively, making it well-suited for modern text data with complex language structures. BERTopic is known for achieving high interpretability, even in short and noisy text data. However, it may require more computational resources due to the use of BERT embeddings. BERTopic is a strong choice when you prioritize topic interpretability and need to capture the nuanced meaning of words and phrases in your text data, making it a valuable option for a wide range of NLP tasks.\n",
        "\n",
        "'''\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "924721d9438e44faa8d0c3d9816af9a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92e22a611ced4540aea7ed59b219484a",
              "IPY_MODEL_9adc1f4c8b694beeac19b65b3f9d2393",
              "IPY_MODEL_f4100725c1614c31af386a9f30d32abe"
            ],
            "layout": "IPY_MODEL_987dbfee7c534fe5af6b3b094c3506c1"
          }
        },
        "92e22a611ced4540aea7ed59b219484a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a42e538a4fa458dbadb5aac813ebce0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dfb9b65fb3774d8ba0a9b251d7e92352",
            "value": "Batches: 100%"
          }
        },
        "9adc1f4c8b694beeac19b65b3f9d2393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4420874b86db4be58bc61cdbcbd5930c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3f48c05ad1840cd9e952a1a841d5b5d",
            "value": 2
          }
        },
        "f4100725c1614c31af386a9f30d32abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d67ac00a27324f27b7bd8218df59ea51",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_36780e88a0c741beaf20d1a3a50d8d54",
            "value": " 2/2 [00:00&lt;00:00,  2.25it/s]"
          }
        },
        "987dbfee7c534fe5af6b3b094c3506c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a42e538a4fa458dbadb5aac813ebce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfb9b65fb3774d8ba0a9b251d7e92352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4420874b86db4be58bc61cdbcbd5930c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3f48c05ad1840cd9e952a1a841d5b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d67ac00a27324f27b7bd8218df59ea51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36780e88a0c741beaf20d1a3a50d8d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}